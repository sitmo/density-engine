{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLflow Model Evaluation\n",
        "\n",
        "This notebook loads the 'adventurous-asp-289' model from MLflow and evaluates it on the test dataset.\n",
        "\n",
        "## Tasks:\n",
        "1. Load the MLflow model\n",
        "2. Evaluate on 100 batches of size 256 from test set\n",
        "3. Plot error PDF histogram\n",
        "4. Plot scatter of absolute error vs individual 8 model inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# Set matplotlib style for better plots\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.rcParams['grid.alpha'] = 0.3\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load MLflow Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set MLflow tracking URI to the local mlruns directory\n",
        "mlflow.set_tracking_uri(\"file:../mlruns\")\n",
        "\n",
        "# Load the specific run\n",
        "run_id = \"b7cf16c3f43b447c86f021363048226d\"\n",
        "run_id = \"27cb8cf4393c4a63adfae778115d7732\"\n",
        "print(f\"Loading model from run: {run_id}\")\n",
        "\n",
        "# Load the model\n",
        "model_uri = f\"runs:/{run_id}/model\"\n",
        "model = mlflow.pytorch.load_model(model_uri)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Model type: {type(model)}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Get device from model\n",
        "device = next(model.parameters()).device\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Prepare Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define column names (same as in train.py)\n",
        "PARAM_COLS = [\"alpha\", \"gamma\", \"beta\", \"var0\", \"eta\", \"lam\", \"ti\", \"ti_indicator\"]\n",
        "ORIGINAL_COLS = [\"alpha\", \"gamma\", \"beta\", \"var0\", \"eta\", \"lam\", \"ti\", \"p\", \"x\"]\n",
        "\n",
        "def build_params_on_device(batch, device):\n",
        "    \"\"\"Build parameters on GPU from raw batch data with transforms applied on device.\"\"\"\n",
        "    # Move all tensors to device first (non-blocking for efficiency)\n",
        "    for k in batch:\n",
        "        batch[k] = batch[k].to(device, non_blocking=True)\n",
        "    \n",
        "    # Apply transforms on GPU\n",
        "    ti_original = batch[\"ti\"]\n",
        "    \n",
        "    params = torch.stack([\n",
        "        batch[\"alpha\"],\n",
        "        batch[\"gamma\"],\n",
        "        batch[\"beta\"],\n",
        "        torch.log(batch[\"var0\"]),                    # log transform on GPU\n",
        "        torch.log(batch[\"eta\"]) - 1.0,               # log transform on GPU\n",
        "        batch[\"lam\"],\n",
        "        torch.log(ti_original - 0.8) - 1.0,          # log transform on GPU\n",
        "        (ti_original == 1.0).float(),                # ti_indicator on GPU\n",
        "    ], dim=1)\n",
        "    \n",
        "    targets = batch[\"x\"]  # already [B, 512]\n",
        "    return params, targets\n",
        "\n",
        "def compute_cdf_metrics(model, params, targets, quantile_levels):\n",
        "    \"\"\"\n",
        "    Compute CDF-based metrics efficiently (single CDF call).\n",
        "    \n",
        "    Args:\n",
        "        model: MDN model\n",
        "        params: Input parameters [B, 8]\n",
        "        targets: Target quantiles [B, 512]\n",
        "        quantile_levels: Reference quantile levels [512]\n",
        "    \n",
        "    Returns:\n",
        "        row_losses: RMSE loss per row [B] for importance sampling\n",
        "        mean_max_diff: Mean of max absolute differences (scalar)\n",
        "    \"\"\"\n",
        "    # Compute CDF values once\n",
        "    cdf_values = model.cdf(params, targets)  # [B, 512]\n",
        "    \n",
        "    # Reference quantile levels - expand to [B, 512]\n",
        "    batch_size = params.size(0)\n",
        "    ref_quantiles = quantile_levels.unsqueeze(0).expand(batch_size, -1)\n",
        "    \n",
        "    # Compute differences\n",
        "    diff = cdf_values - ref_quantiles  # [B, 512]\n",
        "    \n",
        "    # RMSE per row (for loss and outlier detection)\n",
        "    row_losses = torch.sqrt(torch.mean(diff ** 2, dim=1))  # [B]\n",
        "    \n",
        "    # Max absolute difference per row, then mean across batch (for monitoring)\n",
        "    max_diff_per_row = torch.abs(diff).amax(dim=1)  # [B]\n",
        "    mean_max_diff = max_diff_per_row.mean()  # scalar\n",
        "    \n",
        "    return row_losses, mean_max_diff, diff\n",
        "\n",
        "print(\"Data loading functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "print(\"Loading dataset...\")\n",
        "ds = load_dataset(\"sitmo/garch_densities\", token=False)\n",
        "print(f\"Dataset loaded. Available splits: {list(ds.keys())}\")\n",
        "\n",
        "# Create test dataset with PyTorch format\n",
        "test_dataset = ds[\"test\"].with_format(\"torch\", columns=ORIGINAL_COLS)\n",
        "\n",
        "# Create test loader\n",
        "batch_size = 256\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=torch.cuda.is_available(),\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "print(f\"Test dataset: {len(test_dataset):,} samples\")\n",
        "print(f\"Test batches: {len(test_loader):,} batches\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "# Create quantile levels for loss computation\n",
        "quantile_levels = torch.linspace(0.001, 0.999, 512, device=device)\n",
        "print(f\"Quantile levels shape: {quantile_levels.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Evaluate Model on Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on 100 batches\n",
        "num_batches = 100\n",
        "print(f\"Evaluating model on {num_batches} batches of size {batch_size}...\")\n",
        "\n",
        "# Storage for results\n",
        "all_errors = []\n",
        "all_abs_errors = []\n",
        "all_params = []\n",
        "all_row_losses = []\n",
        "all_max_diffs = []\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    for batch_idx, test_batch in enumerate(test_loader):\n",
        "        if batch_idx >= num_batches:\n",
        "            break\n",
        "            \n",
        "        # Build parameters on device\n",
        "        params, targets = build_params_on_device(test_batch, device)\n",
        "        \n",
        "        # Compute metrics\n",
        "        row_losses, max_diff, diff = compute_cdf_metrics(model, params, targets, quantile_levels)\n",
        "        \n",
        "        # Store results\n",
        "        all_errors.append(diff.cpu().numpy())  # [B, 512] - raw differences\n",
        "        all_abs_errors.append(torch.abs(diff).cpu().numpy())  # [B, 512] - absolute differences\n",
        "        all_params.append(params.cpu().numpy())  # [B, 8] - input parameters\n",
        "        all_row_losses.append(row_losses.cpu().numpy())  # [B] - RMSE per row\n",
        "        all_max_diffs.append(max_diff.cpu().numpy())  # scalar\n",
        "        \n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"Processed {batch_idx + 1}/{num_batches} batches\")\n",
        "\n",
        "print(f\"Evaluation completed! Processed {len(all_errors)} batches\")\n",
        "print(f\"Total samples evaluated: {sum(len(err) for err in all_errors):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Concatenate all results\n",
        "errors = np.concatenate(all_errors, axis=0)  # [N, 512]\n",
        "abs_errors = np.concatenate(all_abs_errors, axis=0)  # [N, 512]\n",
        "params = np.concatenate(all_params, axis=0)  # [N, 8]\n",
        "row_losses = np.concatenate(all_row_losses, axis=0)  # [N]\n",
        "\n",
        "print(f\"Concatenated results:\")\n",
        "print(f\"  Errors shape: {errors.shape}\")\n",
        "print(f\"  Abs errors shape: {abs_errors.shape}\")\n",
        "print(f\"  Params shape: {params.shape}\")\n",
        "print(f\"  Row losses shape: {row_losses.shape}\")\n",
        "\n",
        "# Compute summary statistics\n",
        "mean_error = np.mean(errors)\n",
        "std_error = np.std(errors)\n",
        "mean_abs_error = np.mean(abs_errors)\n",
        "std_abs_error = np.std(abs_errors)\n",
        "mean_row_loss = np.mean(row_losses)\n",
        "std_row_loss = np.std(row_losses)\n",
        "\n",
        "print(f\"\\nSummary statistics:\")\n",
        "print(f\"  Mean error: {mean_error:.6f} ± {std_error:.6f}\")\n",
        "print(f\"  Mean absolute error: {mean_abs_error:.6f} ± {std_abs_error:.6f}\")\n",
        "print(f\"  Mean row loss (RMSE): {mean_row_loss:.6f} ± {std_row_loss:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Plot 1: Error PDF Histogram\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flatten all errors for histogram\n",
        "errors_flat = errors.flatten()\n",
        "abs_errors_flat = abs_errors.flatten()\n",
        "\n",
        "print(f\"Flattened errors: {len(errors_flat):,} points\")\n",
        "print(f\"Error range: [{np.min(errors_flat):.6f}, {np.max(errors_flat):.6f}]\")\n",
        "print(f\"Abs error range: [{np.min(abs_errors_flat):.6f}, {np.max(abs_errors_flat):.6f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create figure with subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Model Error Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Define colors\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "\n",
        "# Plot 1: Raw error histogram\n",
        "n, bins, patches = axes[0, 0].hist(errors_flat, bins=np.linspace(-0.002, 0.002, 100), alpha=0.7, density=True, color=colors[0], edgecolor='black', linewidth=0.5)\n",
        "axes[0, 0].axvline(mean_error, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_error:.4f}')\n",
        "axes[0, 0].axvline(mean_error + std_error, color='orange', linestyle='--', alpha=0.7, label=f'+1σ: {mean_error + std_error:.4f}')\n",
        "axes[0, 0].axvline(mean_error - std_error, color='orange', linestyle='--', alpha=0.7, label=f'-1σ: {mean_error - std_error:.4f}')\n",
        "axes[0, 0].set_xlabel('Error (CDF difference)')\n",
        "axes[0, 0].set_ylabel('Density')\n",
        "axes[0, 0].set_title('Raw Error Distribution')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Absolute error histogram\n",
        "n, bins, patches = axes[0, 1].hist(abs_errors_flat, bins=100, alpha=0.7, density=True, color=colors[1], edgecolor='black', linewidth=0.5)\n",
        "axes[0, 1].axvline(mean_abs_error, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_abs_error:.4f}')\n",
        "axes[0, 1].axvline(mean_abs_error + std_abs_error, color='orange', linestyle='--', alpha=0.7, label=f'+1σ: {mean_abs_error + std_abs_error:.4f}')\n",
        "axes[0, 1].set_xlabel('Absolute Error')\n",
        "axes[0, 1].set_ylabel('Density')\n",
        "axes[0, 1].set_title('Absolute Error Distribution')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Log-scale absolute error histogram\n",
        "log_abs_errors = np.log10(np.maximum(abs_errors_flat, 1e-10))  # Avoid log(0)\n",
        "n, bins, patches = axes[1, 0].hist(log_abs_errors, bins=100, alpha=0.7, density=True, color=colors[2], edgecolor='black', linewidth=0.5)\n",
        "axes[1, 0].axvline(np.log10(mean_abs_error), color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_abs_error:.4f}')\n",
        "axes[1, 0].set_xlabel('log₁₀(Absolute Error)')\n",
        "axes[1, 0].set_ylabel('Density')\n",
        "axes[1, 0].set_title('Log-Scale Absolute Error Distribution')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Row loss (RMSE) histogram\n",
        "n, bins, patches = axes[1, 1].hist(row_losses, bins=100, alpha=0.7, density=True, color=colors[3], edgecolor='black', linewidth=0.5)\n",
        "axes[1, 1].axvline(mean_row_loss, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_row_loss:.4f}')\n",
        "axes[1, 1].axvline(mean_row_loss + std_row_loss, color='orange', linestyle='--', alpha=0.7, label=f'+1σ: {mean_row_loss + std_row_loss:.4f}')\n",
        "axes[1, 1].set_xlabel('Row Loss (RMSE)')\n",
        "axes[1, 1].set_ylabel('Density')\n",
        "axes[1, 1].set_title('Row Loss Distribution')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print additional statistics\n",
        "print(f\"\\nError Statistics:\")\n",
        "print(f\"  Raw error - Mean: {mean_error:.6f}, Std: {std_error:.6f}\")\n",
        "print(f\"  Raw error - Min: {np.min(errors_flat):.6f}, Max: {np.max(errors_flat):.6f}\")\n",
        "print(f\"  Abs error - Mean: {mean_abs_error:.6f}, Std: {std_abs_error:.6f}\")\n",
        "print(f\"  Abs error - Min: {np.min(abs_errors_flat):.6f}, Max: {np.max(abs_errors_flat):.6f}\")\n",
        "print(f\"  Row loss - Mean: {mean_row_loss:.6f}, Std: {std_row_loss:.6f}\")\n",
        "print(f\"  Row loss - Min: {np.min(row_losses):.6f}, Max: {np.max(row_losses):.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Plot 2: Scatter of Absolute Error vs Model Inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameter names for plotting\n",
        "param_names = [\"alpha\", \"gamma\", \"beta\", \"log(var0)\", \"log(eta)-1\", \"lam\", \"log(ti-0.8)-1\", \"ti_indicator\"]\n",
        "\n",
        "# Compute mean absolute error per sample (across all 512 quantiles)\n",
        "mean_abs_error_per_sample = np.mean(abs_errors, axis=1)  # [N]\n",
        "\n",
        "print(f\"Mean absolute error per sample shape: {mean_abs_error_per_sample.shape}\")\n",
        "print(f\"Mean absolute error per sample range: [{np.min(mean_abs_error_per_sample):.6f}, {np.max(mean_abs_error_per_sample):.6f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create scatter plots for each input parameter\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "fig.suptitle('Absolute Error vs Model Inputs', fontsize=16, fontweight='bold')\n",
        "\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(8):\n",
        "    ax = axes[i]\n",
        "    if i == 7:\n",
        "        continue\n",
        "    \n",
        "    # Convert parameter values to rank values [0, 1]\n",
        "    param_values = params[:, i]\n",
        "    param_ranks = (np.argsort(np.argsort(param_values)) + 1) / len(param_values)\n",
        "    \n",
        "    # Create scatter plot with C0 color (no gradient)\n",
        "    ax.scatter(param_ranks, mean_abs_error_per_sample, \n",
        "               alpha=0.6, s=1, c='C0', edgecolors='none')\n",
        "    \n",
        "    # Bin the data and compute statistics\n",
        "    n_bins = 20\n",
        "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
        "    \n",
        "    # Compute mean and 98th percentile for each bin\n",
        "    bin_means = []\n",
        "    bin_98th = []\n",
        "    \n",
        "    for j in range(n_bins):\n",
        "        mask = (param_ranks >= bin_edges[j]) & (param_ranks < bin_edges[j + 1])\n",
        "        if j == n_bins - 1:  # Include the last point\n",
        "            mask = (param_ranks >= bin_edges[j]) & (param_ranks <= bin_edges[j + 1])\n",
        "        \n",
        "        if np.sum(mask) > 0:\n",
        "            bin_errors = mean_abs_error_per_sample[mask]\n",
        "            bin_means.append(np.mean(bin_errors))\n",
        "            bin_98th.append(np.percentile(bin_errors, 98))\n",
        "        else:\n",
        "            bin_means.append(np.nan)\n",
        "            bin_98th.append(np.nan)\n",
        "    \n",
        "    # Convert to numpy arrays for plotting\n",
        "    bin_means = np.array(bin_means)\n",
        "    bin_98th = np.array(bin_98th)\n",
        "    \n",
        "    # Plot horizontal lines for mean and 98th percentile\n",
        "    valid_mask = ~np.isnan(bin_means)\n",
        "    if np.any(valid_mask):\n",
        "        ax.plot(bin_centers[valid_mask], bin_means[valid_mask], \n",
        "                color='red', linewidth=2, alpha=0.8)\n",
        "        ax.plot(bin_centers[valid_mask], bin_98th[valid_mask], \n",
        "                color='black', linewidth=2, alpha=0.8)\n",
        "    \n",
        "    # Set labels and title\n",
        "    ax.set_xlabel(f'{param_names[i]} (rank)')\n",
        "    ax.set_ylabel('Mean Absolute Error')\n",
        "    ax.set_title(f'{param_names[i]}')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print correlation coefficients\n",
        "print(\"\\nCorrelation coefficients between inputs and mean absolute error:\")\n",
        "for i, name in enumerate(param_names):\n",
        "    corr = np.corrcoef(params[:, i], mean_abs_error_per_sample)[0, 1]\n",
        "    print(f\"  {name:15s}: {corr:7.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Additional Analysis: Error vs Quantile Level\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze error patterns across quantile levels\n",
        "quantile_levels_np = quantile_levels.cpu().numpy()\n",
        "mean_error_per_quantile = np.mean(abs_errors, axis=0)  # [512]\n",
        "std_error_per_quantile = np.std(abs_errors, axis=0)   # [512]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Define colors\n",
        "blue_color = '#1f77b4'\n",
        "red_color = '#d62728'\n",
        "\n",
        "# Plot 1: Mean error vs quantile level\n",
        "axes[0].plot(quantile_levels_np, mean_error_per_quantile, color=blue_color, linewidth=2, label='Mean')\n",
        "axes[0].fill_between(quantile_levels_np, \n",
        "                     mean_error_per_quantile - std_error_per_quantile,\n",
        "                     mean_error_per_quantile + std_error_per_quantile,\n",
        "                     alpha=0.3, color=blue_color, label='±1σ')\n",
        "axes[0].set_xlabel('Quantile Level')\n",
        "axes[0].set_ylabel('Mean Absolute Error')\n",
        "axes[0].set_title('Error vs Quantile Level')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Log-scale error vs quantile level\n",
        "axes[1].semilogy(quantile_levels_np, mean_error_per_quantile, color=red_color, linewidth=2, label='Mean')\n",
        "axes[1].fill_between(quantile_levels_np, \n",
        "                     np.maximum(mean_error_per_quantile - std_error_per_quantile, 1e-10),\n",
        "                     mean_error_per_quantile + std_error_per_quantile,\n",
        "                     alpha=0.3, color=red_color, label='±1σ')\n",
        "axes[1].set_xlabel('Quantile Level')\n",
        "axes[1].set_ylabel('Mean Absolute Error (log scale)')\n",
        "axes[1].set_title('Error vs Quantile Level (Log Scale)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nError by quantile level:\")\n",
        "print(f\"  Min error at quantile {quantile_levels_np[np.argmin(mean_error_per_quantile)]:.3f}: {np.min(mean_error_per_quantile):.6f}\")\n",
        "print(f\"  Max error at quantile {quantile_levels_np[np.argmax(mean_error_per_quantile)]:.3f}: {np.max(mean_error_per_quantile):.6f}\")\n",
        "print(f\"  Mean error across all quantiles: {np.mean(mean_error_per_quantile):.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary statistics table\n",
        "summary_stats = {\n",
        "    'Metric': [\n",
        "        'Total samples evaluated',\n",
        "        'Mean raw error',\n",
        "        'Std raw error',\n",
        "        'Mean absolute error',\n",
        "        'Std absolute error',\n",
        "        'Mean row loss (RMSE)',\n",
        "        'Std row loss (RMSE)',\n",
        "        'Max absolute error',\n",
        "        'Min absolute error'\n",
        "    ],\n",
        "    'Value': [\n",
        "        f\"{len(errors_flat):,}\",\n",
        "        f\"{mean_error:.6f}\",\n",
        "        f\"{std_error:.6f}\",\n",
        "        f\"{mean_abs_error:.6f}\",\n",
        "        f\"{std_abs_error:.6f}\",\n",
        "        f\"{mean_row_loss:.6f}\",\n",
        "        f\"{std_row_loss:.6f}\",\n",
        "        f\"{np.max(abs_errors_flat):.6f}\",\n",
        "        f\"{np.min(abs_errors_flat):.6f}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(summary_stats)\n",
        "print(\"\\n=== EVALUATION SUMMARY ===\")\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n=== CORRELATION ANALYSIS ===\")\n",
        "print(\"Correlation between input parameters and mean absolute error:\")\n",
        "for i, name in enumerate(param_names):\n",
        "    corr = np.corrcoef(params[:, i], mean_abs_error_per_sample)[0, 1]\n",
        "    print(f\"  {name:20s}: {corr:7.4f}\")\n",
        "\n",
        "print(\"\\n=== QUANTILE-LEVEL ANALYSIS ===\")\n",
        "print(f\"Error varies across quantile levels:\")\n",
        "print(f\"  Lowest error: {np.min(mean_error_per_quantile):.6f} at quantile {quantile_levels_np[np.argmin(mean_error_per_quantile)]:.3f}\")\n",
        "print(f\"  Highest error: {np.max(mean_error_per_quantile):.6f} at quantile {quantile_levels_np[np.argmax(mean_error_per_quantile)]:.3f}\")\n",
        "print(f\"  Error range: {np.max(mean_error_per_quantile) - np.min(mean_error_per_quantile):.6f}\")\n",
        "\n",
        "print(\"\\n=== MODEL PERFORMANCE ===\")\n",
        "if mean_abs_error < 0.01:\n",
        "    print(\"✅ Model shows excellent performance (mean abs error < 0.01)\")\n",
        "elif mean_abs_error < 0.05:\n",
        "    print(\"✅ Model shows good performance (mean abs error < 0.05)\")\n",
        "elif mean_abs_error < 0.1:\n",
        "    print(\"⚠️  Model shows moderate performance (mean abs error < 0.1)\")\n",
        "else:\n",
        "    print(\"❌ Model shows poor performance (mean abs error >= 0.1)\")\n",
        "\n",
        "print(f\"\\nEvaluation completed successfully!\")\n",
        "print(f\"Model: {run_id}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Samples evaluated: {len(errors_flat):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "density-engine-GEC8AyR8-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
