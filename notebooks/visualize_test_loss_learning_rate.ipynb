{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Loss vs Learning Rate Visualization\n",
        "\n",
        "This notebook visualizes the test_loss data from MLflow experiments, plotting test_loss against learning rate on log-log scales.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import glob\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.rcParams['grid.alpha'] = 0.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_mlflow_metrics(metrics_file: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse MLflow metrics file and return DataFrame with timestamp, value, step.\"\"\"\n",
        "    data = []\n",
        "    try:\n",
        "        with open(metrics_file, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 3:\n",
        "                    timestamp = int(parts[0])\n",
        "                    value = float(parts[1])\n",
        "                    step = int(parts[2])\n",
        "                    data.append({\n",
        "                        'timestamp': timestamp,\n",
        "                        'value': value,\n",
        "                        'step': step\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing {metrics_file}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def get_parameter_value(params_file: str) -> float:\n",
        "    \"\"\"Get parameter value from MLflow params file.\"\"\"\n",
        "    try:\n",
        "        with open(params_file, 'r') as f:\n",
        "            return float(f.read().strip())\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {params_file}: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_mlflow_data(mlruns_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load all MLflow runs data into a single DataFrame.\"\"\"\n",
        "    all_data = []\n",
        "    \n",
        "    # Find all run directories\n",
        "    run_dirs = glob.glob(os.path.join(mlruns_path, \"0\", \"*\"))\n",
        "    \n",
        "    for run_dir in run_dirs:\n",
        "        if not os.path.isdir(run_dir):\n",
        "            continue\n",
        "            \n",
        "        run_id = os.path.basename(run_dir)\n",
        "        \n",
        "        # Get learning rate parameters\n",
        "        lrs_file = os.path.join(run_dir, \"params\", \"lrs\")\n",
        "        lre_file = os.path.join(run_dir, \"params\", \"lre\")\n",
        "        \n",
        "        lrs = get_parameter_value(lrs_file)\n",
        "        lre = get_parameter_value(lre_file)\n",
        "        \n",
        "        if lrs is None or lre is None:\n",
        "            continue\n",
        "            \n",
        "        # Parse test_loss metrics\n",
        "        test_loss_file = os.path.join(run_dir, \"metrics\", \"test_loss\")\n",
        "        if os.path.exists(test_loss_file):\n",
        "            test_loss_df = parse_mlflow_metrics(test_loss_file)\n",
        "            if not test_loss_df.empty:\n",
        "                test_loss_df['run_id'] = run_id\n",
        "                test_loss_df['lrs'] = lrs\n",
        "                test_loss_df['lre'] = lre\n",
        "                test_loss_df['learning_rate'] = lrs  # Using lrs as the learning rate\n",
        "                all_data.append(test_loss_df)\n",
        "    \n",
        "    if not all_data:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    return pd.concat(all_data, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the MLflow data\n",
        "mlruns_path = \"../results_train_garch/learning_rate/mlruns\"\n",
        "df = load_mlflow_data(mlruns_path)\n",
        "\n",
        "print(f\"Loaded data from {df['run_id'].nunique()} runs\")\n",
        "print(f\"Total data points: {len(df)}\")\n",
        "print(f\"Learning rate range: {df['learning_rate'].min():.2e} to {df['learning_rate'].max():.2e}\")\n",
        "print(f\"Test loss range: {df['value'].min():.2e} to {df['value'].max():.2e}\")\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get final test loss for each run (last recorded value)\n",
        "final_losses = df.groupby('run_id').agg({\n",
        "    'value': 'last',\n",
        "    'learning_rate': 'first',\n",
        "    'lrs': 'first',\n",
        "    'lre': 'first'\n",
        "}).reset_index()\n",
        "\n",
        "print(f\"Final test losses for {len(final_losses)} runs\")\n",
        "final_losses.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create log-log plot of final test loss vs learning rate\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Final test loss vs learning rate\n",
        "ax1.loglog(final_losses['learning_rate'], final_losses['value'], 'o', alpha=0.7, markersize=8)\n",
        "ax1.set_xlabel('Learning Rate (log scale)')\n",
        "ax1.set_ylabel('Final Test Loss (log scale)')\n",
        "ax1.set_title('Final Test Loss vs Learning Rate')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Add trend line\n",
        "z = np.polyfit(np.log10(final_losses['learning_rate']), np.log10(final_losses['value']), 1)\n",
        "p = np.poly1d(z)\n",
        "ax1.loglog(final_losses['learning_rate'], 10**p(np.log10(final_losses['learning_rate'])), \n",
        "           \"r--\", alpha=0.8, label=f'Trend (slope={z[0]:.2f})')\n",
        "ax1.legend()\n",
        "\n",
        "# Plot 2: All test loss values vs learning rate (colored by run)\n",
        "unique_runs = df['run_id'].unique()\n",
        "colors = plt.cm.tab20(np.linspace(0, 1, len(unique_runs)))\n",
        "\n",
        "for i, run_id in enumerate(unique_runs):\n",
        "    run_data = df[df['run_id'] == run_id]\n",
        "    ax2.loglog(run_data['learning_rate'], run_data['value'], \n",
        "              color=colors[i], alpha=0.6, linewidth=1, label=f'Run {i+1}' if i < 10 else \"\")\n",
        "\n",
        "ax2.set_xlabel('Learning Rate (log scale)')\n",
        "ax2.set_ylabel('Test Loss (log scale)')\n",
        "ax2.set_title('Test Loss Evolution vs Learning Rate')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Only show legend for first 10 runs to avoid clutter\n",
        "if len(unique_runs) <= 10:\n",
        "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a more detailed analysis with learning rate ranges\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Scatter plot with size based on number of steps\n",
        "step_counts = df.groupby('run_id').size()\n",
        "final_losses['step_count'] = final_losses['run_id'].map(step_counts)\n",
        "\n",
        "scatter = axes[0, 0].scatter(final_losses['learning_rate'], final_losses['value'], \n",
        "                            s=final_losses['step_count']*2, alpha=0.7, \n",
        "                            c=final_losses['step_count'], cmap='viridis')\n",
        "axes[0, 0].set_xscale('log')\n",
        "axes[0, 0].set_yscale('log')\n",
        "axes[0, 0].set_xlabel('Learning Rate')\n",
        "axes[0, 0].set_ylabel('Final Test Loss')\n",
        "axes[0, 0].set_title('Final Test Loss vs Learning Rate\\n(Size = Number of Steps)')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter, ax=axes[0, 0], label='Number of Steps')\n",
        "\n",
        "# Plot 2: Learning rate distribution\n",
        "axes[0, 1].hist(np.log10(final_losses['learning_rate']), bins=20, alpha=0.7, edgecolor='black')\n",
        "axes[0, 1].set_xlabel('Log10(Learning Rate)')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('Learning Rate Distribution')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Test loss distribution\n",
        "axes[1, 0].hist(np.log10(final_losses['value']), bins=20, alpha=0.7, edgecolor='black')\n",
        "axes[1, 0].set_xlabel('Log10(Final Test Loss)')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title('Final Test Loss Distribution')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Learning rate vs test loss with trend analysis\n",
        "axes[1, 1].loglog(final_losses['learning_rate'], final_losses['value'], 'o', alpha=0.7)\n",
        "\n",
        "# Fit power law: loss = a * lr^b\n",
        "log_lr = np.log10(final_losses['learning_rate'])\n",
        "log_loss = np.log10(final_losses['value'])\n",
        "z = np.polyfit(log_lr, log_loss, 1)\n",
        "a, b = 10**z[1], z[0]\n",
        "\n",
        "# Plot fitted line\n",
        "lr_range = np.logspace(np.log10(final_losses['learning_rate'].min()), \n",
        "                       np.log10(final_losses['learning_rate'].max()), 100)\n",
        "fitted_loss = a * (lr_range ** b)\n",
        "axes[1, 1].loglog(lr_range, fitted_loss, 'r--', linewidth=2, \n",
        "                  label=f'Power law: loss = {a:.2e} × lr^{b:.2f}')\n",
        "\n",
        "axes[1, 1].set_xlabel('Learning Rate')\n",
        "axes[1, 1].set_ylabel('Final Test Loss')\n",
        "axes[1, 1].set_title('Power Law Fit')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Power law fit: loss = {a:.2e} × lr^{b:.2f}\")\n",
        "print(f\"R² = {np.corrcoef(log_lr, log_loss)[0,1]**2:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a detailed trajectory plot showing how test loss evolves for different learning rates\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Sort runs by learning rate for better visualization\n",
        "sorted_runs = final_losses.sort_values('learning_rate')['run_id'].values\n",
        "\n",
        "# Plot trajectories for each run\n",
        "for i, run_id in enumerate(sorted_runs):\n",
        "    run_data = df[df['run_id'] == run_id].sort_values('step')\n",
        "    lr = run_data['learning_rate'].iloc[0]\n",
        "    \n",
        "    # Color by learning rate (log scale)\n",
        "    color = plt.cm.plasma(np.log10(lr) / (np.log10(final_losses['learning_rate'].max()) - \n",
        "                                         np.log10(final_losses['learning_rate'].min())))\n",
        "    \n",
        "    ax.loglog(run_data['learning_rate'], run_data['value'], \n",
        "              color=color, alpha=0.7, linewidth=1.5, \n",
        "              label=f'LR={lr:.2e}' if i % 3 == 0 else \"\")\n",
        "\n",
        "ax.set_xlabel('Learning Rate (log scale)', fontsize=14)\n",
        "ax.set_ylabel('Test Loss (log scale)', fontsize=14)\n",
        "ax.set_title('Test Loss Trajectories for Different Learning Rates', fontsize=16)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Add colorbar\n",
        "sm = plt.cm.ScalarMappable(cmap=plt.cm.plasma, \n",
        "                          norm=plt.Normalize(vmin=np.log10(final_losses['learning_rate'].min()), \n",
        "                                            vmax=np.log10(final_losses['learning_rate'].max())))\n",
        "sm.set_array([])\n",
        "cbar = plt.colorbar(sm, ax=ax)\n",
        "cbar.set_label('Log10(Learning Rate)', fontsize=12)\n",
        "\n",
        "# Only show some labels to avoid clutter\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "if len(handles) > 10:\n",
        "    ax.legend(handles[::len(handles)//10], labels[::len(handles)//10], \n",
        "              bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "else:\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"=== SUMMARY STATISTICS ===\")\n",
        "print(f\"Number of runs: {len(final_losses)}\")\n",
        "print(f\"Learning rate range: {final_losses['learning_rate'].min():.2e} to {final_losses['learning_rate'].max():.2e}\")\n",
        "print(f\"Final test loss range: {final_losses['value'].min():.2e} to {final_losses['value'].max():.2e}\")\n",
        "print(f\"Mean final test loss: {final_losses['value'].mean():.2e}\")\n",
        "print(f\"Std final test loss: {final_losses['value'].std():.2e}\")\n",
        "print(f\"\\nBest performing run:\")\n",
        "best_run = final_losses.loc[final_losses['value'].idxmin()]\n",
        "print(f\"  Run ID: {best_run['run_id']}\")\n",
        "print(f\"  Learning Rate: {best_run['learning_rate']:.2e}\")\n",
        "print(f\"  Final Test Loss: {best_run['value']:.2e}\")\n",
        "print(f\"  Steps: {best_run['step_count']}\")\n",
        "\n",
        "print(f\"\\nWorst performing run:\")\n",
        "worst_run = final_losses.loc[final_losses['value'].idxmax()]\n",
        "print(f\"  Run ID: {worst_run['run_id']}\")\n",
        "print(f\"  Learning Rate: {worst_run['learning_rate']:.2e}\")\n",
        "print(f\"  Final Test Loss: {worst_run['value']:.2e}\")\n",
        "print(f\"  Steps: {worst_run['step_count']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create log-log plot of train_loss vs step, colored by lrs and lrd\n",
        "def load_train_loss_data(mlruns_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load train_loss data from MLflow runs.\"\"\"\n",
        "    all_data = []\n",
        "    \n",
        "    # Find all run directories\n",
        "    run_dirs = glob.glob(os.path.join(mlruns_path, \"0\", \"*\"))\n",
        "    \n",
        "    for run_dir in run_dirs:\n",
        "        if not os.path.isdir(run_dir):\n",
        "            continue\n",
        "            \n",
        "        run_id = os.path.basename(run_dir)\n",
        "        \n",
        "        # Get learning rate parameters\n",
        "        lrs_file = os.path.join(run_dir, \"params\", \"lrs\")\n",
        "        lrd_file = os.path.join(run_dir, \"params\", \"lrd\")\n",
        "        \n",
        "        lrs = get_parameter_value(lrs_file)\n",
        "        lrd = get_parameter_value(lrd_file)\n",
        "        \n",
        "        if lrs is None or lrd is None:\n",
        "            continue\n",
        "            \n",
        "        # Parse train_loss metrics\n",
        "        train_loss_file = os.path.join(run_dir, \"metrics\", \"train_loss\")\n",
        "        if os.path.exists(train_loss_file):\n",
        "            train_loss_df = parse_mlflow_metrics(train_loss_file)\n",
        "            if not train_loss_df.empty:\n",
        "                train_loss_df['run_id'] = run_id\n",
        "                train_loss_df['lrs'] = lrs\n",
        "                train_loss_df['lrd'] = lrd\n",
        "                all_data.append(train_loss_df)\n",
        "    \n",
        "    if not all_data:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    return pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "# Load train loss data\n",
        "train_df = load_train_loss_data(mlruns_path)\n",
        "print(f\"Loaded train loss data from {train_df['run_id'].nunique()} runs\")\n",
        "print(f\"Total train loss data points: {len(train_df)}\")\n",
        "\n",
        "# Create the log-log plot\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Get unique combinations of lrs and lrd for coloring\n",
        "unique_combinations = train_df[['lrs', 'lrd']].drop_duplicates().sort_values(['lrs', 'lrd'])\n",
        "colors = plt.cm.tab20(np.linspace(0, 1, len(unique_combinations)))\n",
        "\n",
        "# Plot each run\n",
        "for i, (_, combo) in enumerate(unique_combinations.iterrows()):\n",
        "    lrs_val, lrd_val = combo['lrs'], combo['lrd']\n",
        "    run_data = train_df[(train_df['lrs'] == lrs_val) & (train_df['lrd'] == lrd_val)]\n",
        "    \n",
        "    # Group by run_id to handle multiple runs with same lrs/lrd\n",
        "    for run_id, run_group in run_data.groupby('run_id'):\n",
        "        ax.loglog(run_group['step'], run_group['value'], \n",
        "                 color=colors[i], alpha=0.7, linewidth=1.5,\n",
        "                 label=f'lrs={lrs_val:.2e}, lrd={lrd_val:.2e}' if run_id == run_group['run_id'].iloc[0] else \"\")\n",
        "\n",
        "ax.set_xlabel('Step (log scale)', fontsize=14)\n",
        "ax.set_ylabel('Train Loss (log scale)', fontsize=14)\n",
        "ax.set_title('Train Loss vs Step (Log-Log Plot)', fontsize=16)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Add legend (limit to avoid clutter)\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "if len(handles) > 15:\n",
        "    # Show every nth label to avoid overcrowding\n",
        "    step = len(handles) // 15\n",
        "    ax.legend(handles[::step], labels[::step], \n",
        "              bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
        "else:\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary of unique lrs/lrd combinations\n",
        "print(f\"\\nUnique lrs/lrd combinations:\")\n",
        "for _, combo in unique_combinations.iterrows():\n",
        "    count = len(train_df[(train_df['lrs'] == combo['lrs']) & (train_df['lrd'] == combo['lrd'])]['run_id'].unique())\n",
        "    print(f\"  lrs={combo['lrs']:.2e}, lrd={combo['lrd']:.2e}: {count} runs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create plot of learning_rate vs step (linear x-axis, log y-axis)\n",
        "def load_learning_rate_data(mlruns_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load learning_rate data from MLflow runs.\"\"\"\n",
        "    all_data = []\n",
        "    \n",
        "    # Find all run directories\n",
        "    run_dirs = glob.glob(os.path.join(mlruns_path, \"0\", \"*\"))\n",
        "    \n",
        "    for run_dir in run_dirs:\n",
        "        if not os.path.isdir(run_dir):\n",
        "            continue\n",
        "            \n",
        "        run_id = os.path.basename(run_dir)\n",
        "        \n",
        "        # Get learning rate parameters\n",
        "        lrs_file = os.path.join(run_dir, \"params\", \"lrs\")\n",
        "        lrd_file = os.path.join(run_dir, \"params\", \"lrd\")\n",
        "        \n",
        "        lrs = get_parameter_value(lrs_file)\n",
        "        lrd = get_parameter_value(lrd_file)\n",
        "        \n",
        "        if lrs is None or lrd is None:\n",
        "            continue\n",
        "            \n",
        "        # Parse learning_rate metrics\n",
        "        lr_file = os.path.join(run_dir, \"metrics\", \"learning_rate\")\n",
        "        if os.path.exists(lr_file):\n",
        "            lr_df = parse_mlflow_metrics(lr_file)\n",
        "            if not lr_df.empty:\n",
        "                lr_df['run_id'] = run_id\n",
        "                lr_df['lrs'] = lrs\n",
        "                lr_df['lrd'] = lrd\n",
        "                all_data.append(lr_df)\n",
        "    \n",
        "    if not all_data:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    return pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "# Load learning rate data\n",
        "lr_df = load_learning_rate_data(mlruns_path)\n",
        "print(f\"Loaded learning rate data from {lr_df['run_id'].nunique()} runs\")\n",
        "print(f\"Total learning rate data points: {len(lr_df)}\")\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Get unique combinations of lrs and lrd for coloring\n",
        "unique_combinations = lr_df[['lrs', 'lrd']].drop_duplicates().sort_values(['lrs', 'lrd'])\n",
        "colors = plt.cm.tab20(np.linspace(0, 1, len(unique_combinations)))\n",
        "\n",
        "# Plot each run\n",
        "for i, (_, combo) in enumerate(unique_combinations.iterrows()):\n",
        "    lrs_val, lrd_val = combo['lrs'], combo['lrd']\n",
        "    run_data = lr_df[(lr_df['lrs'] == lrs_val) & (lr_df['lrd'] == lrd_val)]\n",
        "    \n",
        "    # Group by run_id to handle multiple runs with same lrs/lrd\n",
        "    for run_id, run_group in run_data.groupby('run_id'):\n",
        "        ax.semilogy(run_group['step'], run_group['value'], \n",
        "                   color=colors[i], alpha=0.7, linewidth=1.5,\n",
        "                   label=f'lrs={lrs_val:.2e}, lrd={lrd_val:.2e}' if run_id == run_group['run_id'].iloc[0] else \"\")\n",
        "\n",
        "ax.set_xlabel('Step', fontsize=14)\n",
        "ax.set_ylabel('Learning Rate (log scale)', fontsize=14)\n",
        "ax.set_title('Learning Rate vs Step (Linear-Log Plot)', fontsize=16)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Add legend (limit to avoid clutter)\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "if len(handles) > 15:\n",
        "    # Show every nth label to avoid overcrowding\n",
        "    step = len(handles) // 15\n",
        "    ax.legend(handles[::step], labels[::step], \n",
        "              bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
        "else:\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary of learning rate evolution\n",
        "print(f\"\\nLearning rate evolution summary:\")\n",
        "for _, combo in unique_combinations.iterrows():\n",
        "    combo_data = lr_df[(lr_df['lrs'] == combo['lrs']) & (lr_df['lrd'] == combo['lrd'])]\n",
        "    initial_lr = combo_data.groupby('run_id')['value'].first().mean()\n",
        "    final_lr = combo_data.groupby('run_id')['value'].last().mean()\n",
        "    print(f\"  lrs={combo['lrs']:.2e}, lrd={combo['lrd']:.2e}: {len(combo_data['run_id'].unique())} runs\")\n",
        "    print(f\"    Initial LR: {initial_lr:.2e}, Final LR: {final_lr:.2e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create 2-column subplot: learning rate vs steps (left) and test_loss vs steps (right)\n",
        "# Focus on lrd=1.00e+00 (no decay) curves in blue, plus best curve (lrs=2E-03, lrd=0.999) in red\n",
        "\n",
        "# First, let's check what columns we have in our dataframes\n",
        "print(\"Columns in df (test_loss data):\", df.columns.tolist())\n",
        "print(\"Columns in lr_df (learning_rate data):\", lr_df.columns.tolist())\n",
        "\n",
        "# We need to add lrd column to df by merging with lr_df or loading it separately\n",
        "# Let's create a mapping from run_id to lrd\n",
        "run_to_lrd = lr_df.groupby('run_id')['lrd'].first().to_dict()\n",
        "run_to_lrs = lr_df.groupby('run_id')['lrs'].first().to_dict()\n",
        "\n",
        "# Add lrd and lrs columns to df\n",
        "df['lrd'] = df['run_id'].map(run_to_lrd)\n",
        "df['lrs'] = df['run_id'].map(run_to_lrs)\n",
        "\n",
        "print(f\"Added lrd and lrs columns to df. Unique lrd values: {df['lrd'].unique()}\")\n",
        "\n",
        "# Filter data for specific curves we want to show\n",
        "# 1. Highest lrs=2E-3 with lrd=1\n",
        "# 2. Lowest lrs=2E-5 with lrd=1  \n",
        "# 3. Current red line: lrs=2E-3, lrd=0.999\n",
        "# 4. New one: lrs=2E-3, lrd=0.998\n",
        "\n",
        "# First, let's check what learning rates are actually available for lrd=1.0\n",
        "available_lrs_lrd1 = sorted(lr_df[lr_df['lrd'] == 1.0]['lrs'].unique())\n",
        "print(f\"Available lrs values for lrd=1.0: {available_lrs_lrd1}\")\n",
        "\n",
        "# Define the specific curves we want - updated with correct values\n",
        "target_curves = [\n",
        "    {'lrs': 1e-3, 'lrd': 1.0, 'label': 'lrs=1E-3, lrd=1.0', 'color': '#1f77b4'},  # blue - highest lrs with no decay\n",
        "    {'lrs': 2e-5, 'lrd': 1.0, 'label': 'lrs=2E-5, lrd=1.0', 'color': '#ff7f0e'},  # orange - lowest lrs with no decay\n",
        "    {'lrs': 2e-3, 'lrd': 0.999, 'label': 'lrs=2E-3, lrd=0.999', 'color': 'red'},  # red - current best\n",
        "    {'lrs': 2e-3, 'lrd': 0.998, 'label': 'lrs=2E-3, lrd=0.998', 'color': '#9467bd'}  # purple - new curve\n",
        "]\n",
        "\n",
        "print(\"Target curves:\")\n",
        "for curve in target_curves:\n",
        "    print(f\"  {curve['label']} - {curve['color']}\")\n",
        "\n",
        "# Filter data for these specific curves\n",
        "filtered_lr_data = []\n",
        "filtered_test_data = []\n",
        "\n",
        "for curve in target_curves:\n",
        "    lrs_val = curve['lrs']\n",
        "    lrd_val = curve['lrd']\n",
        "    \n",
        "    # Get learning rate data\n",
        "    lr_data = lr_df[(lr_df['lrs'] == lrs_val) & (lr_df['lrd'] == lrd_val)]\n",
        "    if not lr_data.empty:\n",
        "        lr_data['curve_label'] = curve['label']\n",
        "        lr_data['curve_color'] = curve['color']\n",
        "        filtered_lr_data.append(lr_data)\n",
        "    \n",
        "    # Get test loss data\n",
        "    test_data = df[(df['lrs'] == lrs_val) & (df['lrd'] == lrd_val)]\n",
        "    if not test_data.empty:\n",
        "        test_data['curve_label'] = curve['label']\n",
        "        test_data['curve_color'] = curve['color']\n",
        "        filtered_test_data.append(test_data)\n",
        "\n",
        "print(f\"\\nFound data for {len(filtered_lr_data)} learning rate curves and {len(filtered_test_data)} test loss curves\")\n",
        "\n",
        "# Create subplot\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(8, 6))\n",
        "\n",
        "# Get unique lrs values for no-decay curves, sorted\n",
        "unique_lrs_no_decay = sorted(no_decay_lr_data['lrs'].unique())\n",
        "print(f\"Unique lrs values (no decay): {unique_lrs_no_decay}\")\n",
        "\n",
        "# Create better color scheme with blue, purple, green, orange\n",
        "n_colors = len(unique_lrs_no_decay)\n",
        "# Create a custom colormap that transitions through blue -> purple -> green -> orange\n",
        "colors_list = []\n",
        "if n_colors == 1:\n",
        "    colors_list = ['#1f77b4']  # blue\n",
        "elif n_colors == 2:\n",
        "    colors_list = ['#1f77b4', '#ff7f0e']  # blue, orange\n",
        "elif n_colors == 3:\n",
        "    colors_list = ['#1f77b4', '#9467bd', '#ff7f0e']  # blue, purple, orange\n",
        "elif n_colors == 4:\n",
        "    colors_list = ['#1f77b4', '#9467bd', '#2ca02c', '#ff7f0e']  # blue, purple, green, orange\n",
        "else:\n",
        "    # For more colors, create a gradient through the desired colors\n",
        "    import matplotlib.colors as mcolors\n",
        "    # Define custom colors: blue -> purple -> green -> orange\n",
        "    custom_colors = ['#1f77b4', '#9467bd', '#2ca02c', '#ff7f0e']\n",
        "    cmap = mcolors.LinearSegmentedColormap.from_list('custom', custom_colors)\n",
        "    colors_list = [cmap(i / (n_colors - 1)) for i in range(n_colors)]\n",
        "\n",
        "print(f\"Using {len(colors_list)} colors: {colors_list}\")\n",
        "\n",
        "# Plot no-decay curves in blue gradients\n",
        "# Plot the specific curves\n",
        "for i, curve_data in enumerate(filtered_lr_data):\n",
        "    curve_label = curve_data['curve_label'].iloc[0]\n",
        "    curve_color = curve_data['curve_color'].iloc[0]\n",
        "    \n",
        "    # Learning rate plot (left)\n",
        "    for run_id, run_group in curve_data.groupby('run_id'):\n",
        "        ax2.semilogy(run_group['step'], run_group['value'], \n",
        "                    color=curve_color, alpha=0.8, linewidth=1,\n",
        "                    label=curve_label if run_id == run_group['run_id'].iloc[0] else \"\")\n",
        "    \n",
        "    # Bottom right: Learning rate plot (loglog)\n",
        "    for run_id, run_group in curve_data.groupby('run_id'):\n",
        "        ax4.loglog(run_group['step'], run_group['value'], \n",
        "                  color=curve_color, alpha=0.8, linewidth=1,\n",
        "                  label=curve_label if run_id == run_group['run_id'].iloc[0] else \"\")\n",
        "\n",
        "# Plot test loss curves\n",
        "for i, curve_data in enumerate(filtered_test_data):\n",
        "    curve_label = curve_data['curve_label'].iloc[0]\n",
        "    curve_color = curve_data['curve_color'].iloc[0]\n",
        "    \n",
        "    # Top left: Test loss plot (semilogy)\n",
        "    for run_id, run_group in curve_data.groupby('run_id'):\n",
        "        ax1.semilogy(run_group['step'], run_group['value'], \n",
        "                    color=curve_color, alpha=0.8, linewidth=1,\n",
        "                    label=curve_label if run_id == run_group['run_id'].iloc[0] else \"\")\n",
        "    \n",
        "    # Bottom left: Test loss plot (loglog)\n",
        "    for run_id, run_group in curve_data.groupby('run_id'):\n",
        "        ax3.loglog(run_group['step'], run_group['value'], \n",
        "                  color=curve_color, alpha=0.8, linewidth=1,\n",
        "                  label=curve_label if run_id == run_group['run_id'].iloc[0] else \"\")\n",
        "\n",
        "        ax3.loglog(run_group['step'], run_group['value'], \n",
        "                  color=curve_color, alpha=0.8,\n",
        "                  label=curve_label if run_id == run_group['run_id'].iloc[0] else \"\")\n",
        "\n",
        "\n",
        "# Configure plots\n",
        "# Top left: Test loss vs steps (semilogy)\n",
        "ax1.set_xlabel('Steps')\n",
        "#ax1.set_ylabel('Test Loss (log scale)', fontsize=10)\n",
        "ax1.set_title('Test Loss (Semi-Log)')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Top right: Learning rate vs steps (semilogy)\n",
        "ax2.set_xlabel('Steps')\n",
        "#ax2.set_ylabel('Learning Rate (log scale)', fontsize=10)\n",
        "ax2.set_title('Learning Rate (Semi-Log)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Bottom left: Test loss vs steps (loglog)\n",
        "ax3.set_xlabel('Steps')\n",
        "#ax3.set_ylabel('Test Loss (log scale)', fontsize=10)\n",
        "ax3.set_title('Test Loss (Log-Log)',)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Bottom right: Learning rate vs steps (loglog)\n",
        "ax4.set_xlabel('Steps')\n",
        "#ax4.set_ylabel('Learning Rate', fontsize=10)\n",
        "ax4.set_title('Learning Rate (Log-Log)')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Remove top and right spines from all plots\n",
        "for ax in [ax1, ax2, ax3, ax4]:\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "\n",
        "# Add legend only to top right plot (learning rate semilogy)\n",
        "handles, labels = ax2.get_legend_handles_labels()\n",
        "#ax2.legend(handles, labels, loc='upper right', \n",
        "#          frameon=True, fancybox=True, shadow=True, \n",
        "#          facecolor='white', edgecolor='none', framealpha=0.8, fontsize=8)\n",
        "\n",
        "\n",
        "# Configure right plot (test loss)\n",
        "ax3.set_xlabel('Steps')\n",
        "#ax3.set_ylabel('Test Loss')\n",
        "ax3.set_title('Test Loss (Log-Log)')\n",
        "ax3.grid(True, alpha=0.5)\n",
        "\n",
        "\n",
        "# Add legend only to left plot, no frame, with semi-transparent background\n",
        "handles, labels = ax1.get_legend_handles_labels()\n",
        "\n",
        "# Reverse the order of blue entries (highest learning rate at top)\n",
        "# Separate blue and red entries\n",
        "blue_handles = []\n",
        "blue_labels = []\n",
        "red_handles = []\n",
        "red_labels = []\n",
        "\n",
        "for handle, label in zip(handles, labels):\n",
        "    if 'Best:' in label:\n",
        "        red_handles.append(handle)\n",
        "        red_labels.append(label)\n",
        "    else:\n",
        "        blue_handles.append(handle)\n",
        "        blue_labels.append(label)\n",
        "\n",
        "# Reverse blue entries and combine with red at bottom\n",
        "reversed_handles = blue_handles[::-1] + red_handles\n",
        "reversed_labels = blue_labels[::-1] + red_labels\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('leanring_rate.png', dpi=300)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "density-engine-GEC8AyR8-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
